{"cells":[{"cell_type":"markdown","metadata":{"id":"ln2BJkr-iVlr"},"source":["![LogoUC3M](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a6/Acr%C3%B3nimo_y_nombre_de_la_UC3M.svg/320px-Acr%C3%B3nimo_y_nombre_de_la_UC3M.svg.png)\n","\n","### Machine Learning · Bachelor in Management and Technology\n","# Tutorial 1: K-Nearest Neighbors with Scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"oNnyFPspiVlr"},"source":["We will install the following library:\n","\n","+ `statsmodels`: for confidence intervals.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R2rNJKxq1fTI"},"outputs":[],"source":["!pip install statsmodels"]},{"cell_type":"markdown","metadata":{"id":"cWAOeLZ94XOz"},"source":["# Data Structures in Python\n","\n","- Basic Python: lists, dictionaries, sets, ...\n","- Numpy: numeric vectors and matrices.\n","- Pandas: Dataframes (tables)"]},{"cell_type":"markdown","metadata":{"id":"MS7sU2en7dVC"},"source":["This is a \"basic list\" in Python, a collection of items (of any kind)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfuwVwX65nqi"},"outputs":[],"source":["print('This is a Python list:')\n","a = [1, 2, 3]\n","print(a)"]},{"cell_type":"markdown","metadata":{"id":"ZAEx9NdA7jby"},"source":["These are a Numpy vector and a Numpy matrix. They must hold numeric values."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9kybjyy77jra"},"outputs":[],"source":["import numpy as np\n","\n","print('This is a Numpy vector:')\n","a = np.array([1, 2, 3])\n","print(a)\n","\n","print('\\n-----\\n')\n","\n","print('This is a Numpy matrix:')\n","b = np.array([[1,2,3],\n","              [4,5,6]])\n","print(b)"]},{"cell_type":"markdown","metadata":{"id":"m1NceMWE8FDD"},"source":["This is a Pandas dataframe. It stores a table of data (similar to SQL relations), with different columns, which can have different types (number, strings, categorical values, etc)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mt5NzVA18FM6"},"outputs":[],"source":["import pandas as pd\n","\n","print('This is a Pandas dataframe:')\n","df = pd.DataFrame({\n","    'a': [1,2,3],\n","    'b': ['a', 'b', 'c']\n","})\n","display(df)"]},{"cell_type":"markdown","metadata":{"id":"E2iDVRYc9fFi"},"source":["Pandas dataframes are a very flexible and appropriate structure for representing data.\n","\n","Some libraries might only work with Numpy arrays, but there are simple ways to convert a Pandas dataframe to a matrix. To do so, categorical values must be encoded as numbers.\n","\n","A typical workflow for machine learning can be the following:\n","\n","1. Load data as a Pandas dataframe\n","2. Do exploratory data analysis (EDA) to understand your data\n","3. Encode the Pandas dataframe as a numpy matrix (get rid of categorical values and missing values)\n","4. Do machine learning"]},{"cell_type":"markdown","metadata":{"id":"J9dwWGrb4QXi"},"source":["# Scikit-learn\n","\n","Scikit-learn (`sklearn`) is a Python library that provides a collection of machine learning algorithms and tools.\n","https://scikit-learn.org/stable/"]},{"cell_type":"markdown","metadata":{"id":"DL5_Er0v4QXl"},"source":["# Input data in Scikit-learn"]},{"cell_type":"markdown","metadata":{"id":"m-3u_OGV4QXm"},"source":["Some algorithms require that input data be **numeric** matrices. In such cases, this implies that categorical attributes must be represented as:\n","+ Integers\n","+ One-hot encoding / dummy variables.\n","\n","However, other algorithms will work with Pandas dataframes with categorical values.\n","\n","Missing values can also be represented as `NaN`."]},{"cell_type":"markdown","metadata":{"id":"iMtbmEyl4QXm"},"source":["# The Iris flower dataset\n","\n","The *Iris flower data set* was introduced by the British statistician and biologist Ronald Fisher in 1936, and has since been widely used as a benchmark for validating classification models.\n","\n","The dataset contains measurements of *iris* flowers belonging to three different species: *setosa*, *versicolor* and *virginica*.\n","\n","The following image shows an example of each kind:\n","\n","![](https://miro.medium.com/max/2550/0*GVjzZeYrir0R_6-X.png)\n","\n","The dataset contains 150 samples (**instances**), 50 for each of the species (**classes**). Besides the class label, each sample contains the following information (**attributes** or **features**):\n","\n","*   Petal length (cm.)\n","*   Petal width (cm.)\n","*   Sepal length (cm.)\n","*   Sepal width (cm.)\n","\n","Therefore, the data structure is the following:\n","\n","![](https://bishwamittra.github.io/images/imli/iris_dataset.png)\n","\n","The Iris dataset is available directly from Scikit-learn. Otherwise, we could load it from a CSV or Excel file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwhF9rEr4QXm"},"outputs":[],"source":["from sklearn.datasets import load_iris\n","iris = load_iris()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i4XLUUY-4QXn"},"outputs":[],"source":["print(\"Features of the Iris dataset:\")\n","print(iris.feature_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BraxjcIT4QXo"},"outputs":[],"source":["print(\"Class labels of the Iris dataset:\")\n","print(iris.target_names)"]},{"cell_type":"markdown","metadata":{"id":"IUWlrFzX4QXp"},"source":["We will retrieve the data from the Iris dataset. In many cases, we will adhere to the following convention:\n","\n","+ `X` is the matrix of attributes (samples as rows, features as columns).\n","+ `y` is the vector of class labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wETZD6q64QXp"},"outputs":[],"source":["X = iris.data\n","y = iris.target"]},{"cell_type":"markdown","metadata":{"id":"eTfFws3S4QXq"},"source":["We can see that `X` (the input features) is a 2-dimensional Numpy array, and `y` (the response variable) is a Numpy vector."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tTf1ylam4QXq"},"outputs":[],"source":["print(type(X))\n","print(X.shape) # 150 instances and 4 input features\n","print(X.dtype) # Values are real numbers (float)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xQb5GqPa4QXq"},"outputs":[],"source":["print(type(y))\n","print(y.shape) # 150 values of the response variable\n","print(y.dtype) # Values are integers"]},{"cell_type":"markdown","metadata":{"id":"I1MSNYt04QXr"},"source":["Let's take a look at the first 10 instances:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-UOb_pJx4QXr"},"outputs":[],"source":["X[0:10,0:4]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Oa8xhgG34QXr"},"outputs":[],"source":["y[0:10]"]},{"cell_type":"markdown","metadata":{"id":"GL8zQzuh4QXr"},"source":["Below, we can see the whole table, with the response variable being the last column.\n","It is not necessary to do this when working with Scikit-learn, it is just for visualization purposes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LASHXWc74QXs"},"outputs":[],"source":["np.concatenate((X, y[:,np.newaxis]), axis=1)[0:10]"]},{"cell_type":"markdown","metadata":{"id":"rke_0WUy4QXs"},"source":["We can plot the Iris flower dataset to see how it looks like:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SUtaFKIH4QXs"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Paired)\n","plt.xlabel('Sepal length')\n","plt.ylabel('Sepal width')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"C1HKbQlo4QXs"},"source":["# Training a K-Nearest Neighbors classifier"]},{"cell_type":"markdown","metadata":{"id":"JduA5Vd2iVlu"},"source":["We will follow the steps to train (fit) a KNN classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9zidraxu4QXt"},"outputs":[],"source":["from sklearn import neighbors\n","\n","# Here, we define the type of training method (nothing happens yet)\n","clf = neighbors.KNeighborsClassifier()\n","\n","# Now, we train (fit) the method on the (X,y) dataset\n","clf.fit(X, y)\n","\n","# clf **has been changed** and now contains the trained model"]},{"cell_type":"markdown","metadata":{"id":"bDexRXHP4QXu"},"source":["# Training and evaluating a classifier with a test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O7uNOyNv4QXu"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"markdown","metadata":{"id":"f-4APKeP4QXu"},"source":["We will create the training (X_train, y_train) and testing (X_test, y_test) sets:\n","- 2/3 for training\n","- 1/3 for testing\n","\n","Notice that we set a **random_state** for reproducibility (this is important!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSfsVFxH4QXu"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"L1OxaH3-4QXv"},"source":["Let's check the shapes of the training and testing partitions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLayO_Ir4QXv"},"outputs":[],"source":["print(X_train.shape, y_train.shape) # 100 instances for training\n","print(X_test.shape, y_test.shape)   # 50 instances for testing"]},{"cell_type":"markdown","metadata":{"id":"Fxjyn5es4QXv"},"source":["Let's print the five first training instances:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jS3dEvOK4QXv"},"outputs":[],"source":["print(\"Input features:\")\n","print(X_train[:5,:])\n","\n","print(\"\\n-----\\n\")\n","\n","print(\"Classes:\")\n","print(y_train[:5])"]},{"cell_type":"markdown","metadata":{"id":"M-hCi0G-4QXv"},"source":["If we create the partition once again, it will be the same as before if we use the same random state:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EMUbPInn4QXv"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","print(\"Input features:\")\n","print(X_train[:5,:])"]},{"cell_type":"markdown","metadata":{"id":"PJwTUDiv4QXv"},"source":["But it will be different if we change the random seed:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C9iUH1c14QXw"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=24)\n","\n","print(\"Input features:\")\n","print(X_train[:5,:])"]},{"cell_type":"markdown","metadata":{"id":"yNbslxjdiVlx"},"source":["Let's keep the original partition (with random state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GwiXO5WR4QXw"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"2W36mNPh4QXw"},"source":["Now, we will train the classifier with the `fit` method, only using the training set. We will also use scaling to normalize the data, avoding some features to have more weight than others."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGUABN5a4QXw"},"outputs":[],"source":["from sklearn.preprocessing import MinMaxScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.pipeline import Pipeline\n","\n","# Here, we create the scaler\n","scaler = MinMaxScaler()\n","\n","# Here, we set our model to KNN classifier\n","clf = KNeighborsClassifier()\n","\n","# We create a pipeline that first scales the data and then trains the model\n","pipe = Pipeline([\n","    ('scaler', scaler),\n","    ('knn', clf)]\n",")\n","\n","# Now, we train it\n","pipe.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"tsF1kKuQ4QXw"},"source":["By the way, we can get help of any function (like `fit`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"my616HwN4QXw"},"outputs":[],"source":["?clf.fit"]},{"cell_type":"markdown","metadata":{"id":"2jqPY0dT4QXw"},"source":["Now, we evaluate the model, by computing predictions on the test set. By using the pipeline, the data is automatically scaled before making predictions."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eCGTo5oO4QXx"},"outputs":[],"source":["y_pred = pipe.predict(X_test)"]},{"cell_type":"markdown","metadata":{"id":"bfu7AeHg4QXx"},"source":["We can check the predictions for the testing instances"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VEzdzi7_4QXx"},"outputs":[],"source":["print(y_pred)"]},{"cell_type":"markdown","metadata":{"id":"1qgkCQxm4QXx"},"source":["For the sake of visualization, we can compare predictions and actual values (ground truth of the response variable). We can see that for the first 5 instances, it is always correct."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K2BLDm24QXx"},"outputs":[],"source":["print(np.hstack((y_pred[:5,np.newaxis], y_test[:5,np.newaxis])))"]},{"cell_type":"markdown","metadata":{"id":"NCpZW27s4QXx"},"source":["But, in order to evaluate the model on the test partition, we can compute a metric (classification accuracy in this case). It looks pretty high (98%)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uspUaRJe4QXx"},"outputs":[],"source":["from sklearn import metrics\n","\n","accuracy_knn = metrics.accuracy_score(y_test, y_pred)\n","print(accuracy_knn)"]},{"cell_type":"markdown","metadata":{"id":"CjgSbJvq4QXx"},"source":["However, the 0.98 accuracy is the model evaluation (estimation of performance). We still need to compute the final model (the one that will be sent and used by the company) **using all available data**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VN0n0ZU94QXy"},"outputs":[],"source":["# Now, we create the pipeline and create it.\n","final_pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","final_pipe.fit(X, y)\n","\n","# final_pipe contains the model that would be used by the company\n","# Its estimated accuracy is what we computed before (98%)"]},{"cell_type":"markdown","metadata":{"id":"8Bzm7znT4QXy"},"source":["By the way, we can store (and load) this model on a file. This is called _model persistence_.\n","\n","> Add blockquote\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V_ukcKMF4QXy"},"outputs":[],"source":["from joblib import dump, load\n","\n","# Save the final model to a file\n","dump(final_pipe, 'final_model.joblib')\n","\n","# Load the model from the file\n","final_clf_reloaded = load('final_model.joblib')"]},{"cell_type":"markdown","metadata":{"id":"ObJu3av94QXy"},"source":["The whole process is summarized in the following cell:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv2Zgzb94QXy"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import neighbors\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Here, we create the classification pipeline\n","pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","\n","# We train the model\n","pipe.fit(X_train, y_train)\n","\n","# We obtain predictions on the test set\n","y_pred = pipe.predict(X_test)\n","\n","# We compute accuracy\n","\n","accuracy_knn = metrics.accuracy_score(y_test, y_pred)\n","print(f\"Accuracy of the model: {accuracy_knn} \")\n","\n","# We finally compute the final model with all available data\n","final_pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","final_pipe.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"f7lsOqmTBnzt"},"source":["# Exercise\n","## Fit, predict, evaluate, and build final model using decision trees.\n","\n","The following class can be used for classification:\n","\n","```\n","from sklearn.tree import DecisionTreeClassifier\n","```\n","\n","The following class can be used for regression:\n","\n","```\n","from sklearn.tree import DecisionTreeRegressor\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSb9cSjfNafr"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","\n","# @TODO Import model\n","\n","#\n","\n","\n","# @TODO Train/test split\n","# Hint: do we need to do something else?\n","\n","\n","\n","#\n","\n","\n","# @TODO Create the decision tree model (no training yet)\n","\n","#\n","\n","\n","# Fix the seed for reproducibility\n","np.random.seed(42)\n","\n","\n","# @TODO Fit the decision tree model\n","\n","#\n","\n","\n","# @TODO Compute predictions in the test set\n","\n","#\n","\n","\n","# @ TODO Compute and print accuracy of the decision tree model\n","\n","\n","#\n","\n","\n","np.random.seed(42)\n","# @ TODO Build final model\n","\n","\n","#"]},{"cell_type":"markdown","metadata":{"id":"hX9_a4E94QXy"},"source":["# Training and evaluating a KNN model with cross validation"]},{"cell_type":"markdown","metadata":{"id":"HiMzev7a4QX0"},"source":["First, we are going to do cross-validation with a loop, so that we understand the process better.\n","\n","**However**, it is better to do cross validation with the `cross_val_score` function, as we will do later."]},{"cell_type":"markdown","metadata":{"id":"r121gQdC4QX0"},"source":["KFold creates the training/test cross validation folds.\n","- The parameter `shuffle` randomly shuffles the data before splitting the folds. We should always do this, unless we have good reasons otherwise.\n","- `random_state` makes the shuffling reproducible."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ECsSw2by4QX0"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"9bOHa6wQ4QX0"},"source":["Now, we carry out cross validation by going through all 5 folds.\n","\n","In every iteration:\n","- We train a model on the training folds\n","- We compute predictions on the testing folds\n","- We compute the metric (accuracy) and store it\n","\n","When the cross validation loop ends, we compute the average and standard deviation.    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wI_3sDaF4QX0"},"outputs":[],"source":["# This variable will contain the 5 cross validation accuracies, one per iteration\n","scores = []\n","\n","for train_index, test_index in cv.split(X):\n","    print(f\"Train: {train_index[:5]} ...\", f\"Test: {test_index[:5]} ...\")\n","    # Getting the actual training and testing partitions out of the indices\n","    X_train, X_test = X[train_index,:], X[test_index,:]\n","    y_train, y_test = y[train_index], y[test_index]\n","\n","    # Training the model for this particular cross validation iteration\n","    pipe = Pipeline([\n","        ('scaler', MinMaxScaler()),\n","        ('knn', KNeighborsClassifier())]\n","    )\n","    pipe.fit(X_train, y_train)\n","    y_test_pred = pipe.predict(X_test)\n","    accuracy_knn = metrics.accuracy_score(y_test, y_test_pred)\n","\n","    print(f\"The accuracy for this cross val iteration is: {accuracy_knn}\")\n","    print()\n","\n","    # We add this accuracy to the list\n","    scores.append(accuracy_knn)\n","\n","# Transforming scores from list to numpy array (this is just a technicality)\n","scores = np.array(scores)\n","print(f\"All the accuracies are: {scores}\")\n","print(f\"The average cross validation accuracy is: {scores.mean():.2f} ± {scores.std():.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"v7dfsh8t4QX0"},"source":["So far, we have programmed the loop explicitly, in order to understand what cross validation does.\n","\n","However, cross validation is typically done by means of the `cross_val_score` function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ejuZVow54QX1"},"outputs":[],"source":["from sklearn.model_selection import cross_val_score, KFold\n","\n","# Create a k-fold cross validation iterator of k=5 folds\n","# shuffle = True randomly rearranges the dataframe\n","# random_state = 42 is for making the folds reproducible\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","\n","scores = cross_val_score(pipe, X, y, scoring='accuracy', cv=cv)\n","\n","print(f\"All the accuracies are: {scores}\")\n","print(f\"The average cross validation accuracy is: {scores.mean():.2f} ± {scores.std():.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"INNQMn8n4QX1"},"source":["We have found that 0.97 is the model evaluation (estimation of performance). Still, the final model has to be trained with all available data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yNXty5034QX1"},"outputs":[],"source":["final_pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","\n","# Now, we train it\n","final_pipe.fit(X, y)\n","\n","# final_pipe contains the model that would be used by the company\n","# Its estimated accuracy is what we computed before"]},{"cell_type":"markdown","metadata":{"id":"prQCnIMt4QX1"},"source":["Below, you have the complete code for cross validation evaluation (and also obtaining the final model at the end):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZT8P6pJz4QX1"},"outputs":[],"source":["from sklearn import neighbors\n","from sklearn.model_selection import KFold, cross_val_score\n","\n","# Create a k-fold cross validation iterator of k=5 folds\n","# shuffle = True randomly rearranges the dataframe\n","# random_state = 42 is for making the folds reproducible\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","scores = cross_val_score(pipe, X, y, scoring='accuracy', cv=cv)\n","\n","print(f\"The average cross validation accuracy is: {scores.mean():.2f} ± {scores.std():.2f}\")\n","\n","final_pipe = Pipeline([\n","    ('scaler', MinMaxScaler()),\n","    ('knn', KNeighborsClassifier())]\n",")\n","\n","# Now, we train it\n","final_pipe.fit(X, y)\n","\n","# final_pipe contains the model that would be used by the company"]},{"cell_type":"markdown","metadata":{"id":"q7FP2q4V4QX1"},"source":["# Changing hyperparameters of a KNN model"]},{"cell_type":"markdown","metadata":{"id":"ewZT0QkW4QX1"},"source":["Take a look at the documentation of the KNN classifier: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Q_jHJgb4QX1"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import neighbors\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"]},{"cell_type":"markdown","metadata":{"id":"C0eoV0e54QX2"},"source":["Let's see the effect of changing the value of the parameter `p` between `1` and `2`. We use holdout here. It seems that results are exactly the same."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eN-4gAJg4QX2"},"outputs":[],"source":["# This loop checks what happens with the two criterions\n","for p in [1, 2]:\n","    pipe = Pipeline([\n","        ('scaler', MinMaxScaler()),\n","        ('knn', KNeighborsClassifier(p=p))]\n","    )\n","    pipe.fit(X_train,y_train)\n","    y_test_pred = pipe.predict(X_test)\n","    accuracy_knn = metrics.accuracy_score(y_test, y_test_pred)\n","    print(f\"With {p=}: {accuracy_knn:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"KgKDmuV84QX2"},"source":["Let's see the effects of `n_neighbors`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UHjavMZk4QX2"},"outputs":[],"source":["for n_neighbors in [2, 4, 8, 16, 32]:\n","    pipe = Pipeline([\n","        ('scaler', MinMaxScaler()),\n","        ('knn', KNeighborsClassifier(n_neighbors=n_neighbors))]\n","    )\n","    pipe.fit(X_train, y_train)\n","    y_test_pred = pipe.predict(X_test)\n","    accuracy_knn = metrics.accuracy_score(y_test, y_test_pred)\n","    print(f\"With {n_neighbors=}: {accuracy_knn:.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"qZ1FfR1l4QX2"},"source":["It seems that a very large value of `n_neighbors` work worse."]},{"cell_type":"markdown","metadata":{"id":"RAJYiJr1OYmz"},"source":["# Exercise\n","## Check the effect of changing hyperparameters in a KNN classifier.\n","\n","Take a look at the documentation of the KNN classifier: https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n","\n","Let's test different values for `weights` (`uniform` and `distance`)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BnCg6h_MCPM4"},"outputs":[],"source":["from sklearn.tree import DecisionTreeClassifier\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# @TODO Define loop for possible values of `weights`\n","\n","#\n","\n","\n","    # @TODO Create the KNN model (no training yet)\n","    # Use a pipeline including a scaler and the KNN model\n","    # Remember to set the desired hyperparameter!\n","\n","    #\n","\n","\n","    # @TODO Fit the tree model\n","\n","    #\n","\n","\n","    # @TODO Compute predictions in the test set\n","\n","    #\n","\n","\n","    # @ TODO Compute and print accuracy of the KNN model\n","\n","\n","    #"]},{"cell_type":"markdown","metadata":{"id":"ZFONncXU4QX3"},"source":["# Dealing with categorical variables\n","\n","Scikit-learn implementation of classification models **cannot** deal with categorical variables (in most cases).\n","- They must be converted to dummy variables (one-hot-encoding)\n","- Most Scikit-learn models cannot deal with missing values either\n","\n","The typical workflow when working with Scikit-learn is:\n","\n","1. Load data as a Pandas dataframe\n","\n","2. Do EDA (Exploratory Data Analysis) to understand your data. And this means:\n","  - How many instances and attributes there are\n","  - What type of attributes there are (numerical or categorical). This is done to check whether there are categorical features that should be encoded (as dummies / one-hot-encoding)\n","  - What attributes have missing values, and how many\n","  - Whether it is a classification or a regression problem (response variable), and in case of classification, whether the class is imbalanced.\n","  \n","3. Encode the Pandas dataframe as a numpy matrix (get rid of categorical values and missing values)\n","\n","4. Do machine learning!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5_UHlRVJqtw"},"outputs":[],"source":["# This is for uploading tennis.txt from your hard drive into Colab\n","from google.colab import files\n","import io\n","uploaded = files.upload()\n","tennis_tmp = io.BytesIO(uploaded['tennis.txt'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vGwW0YWM4QX3"},"outputs":[],"source":["import pandas as pd\n","tennis_df = pd.read_csv(\"tennis.txt\", sep=\",\")"]},{"cell_type":"markdown","metadata":{"id":"8-jAr-k5KQfe"},"source":["We can check the first instances of sky with head"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6fyz7ae4QX3"},"outputs":[],"source":["tennis_df.head()"]},{"cell_type":"markdown","metadata":{"id":"_mdqmMcdKYcl"},"source":["With this dataset is very small, so we can visualize it all:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGckFPDlKdc2"},"outputs":[],"source":["tennis_df"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vw_EJnkwLD9d"},"outputs":[],"source":["print('The shape of the data table is:')\n","print('===============================')\n","print(tennis_df.shape)\n","print()\n","\n","print('The types of the attributes are:')\n","print('================================')\n","tennis_df.info()\n","\n","print()\n","\n","print('How many missing values per attribute:')\n","print('======================================')\n","print(tennis_df.isnull().sum())\n","\n","print()\n","\n","print('Fraction of missing values per attribute:')\n","print('======================================')\n","print(tennis_df.isnull().mean())\n"]},{"cell_type":"markdown","metadata":{"id":"_WpY6Yu8PPFm"},"source":["Finally, we check whether the response variable is imbalanced. We can see it is not too imbalanced."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rG4XjXvQPJ61"},"outputs":[],"source":["print(tennis_df['Play'].value_counts())\n","\n","print()\n","\n","print(tennis_df['Play'].value_counts() / tennis_df['Play'].count())"]},{"cell_type":"markdown","metadata":{"id":"6T8D5USxa34S"},"source":["Now, we are going to encode:\n","- Our categorical features (Sky and Windy)\n","- The response variable (the class: Play)\n","\n","But first, we will separate the data table into inputs (X) and output (y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XLHZORsa0yz"},"outputs":[],"source":["X_df = tennis_df.drop('Play', axis=1)\n","\n","print('X:')\n","print('==========================================')\n","print(X_df)\n","\n","print()\n","\n","print('y:')\n","print('=========')\n","y_df = tennis_df['Play']\n","print(y_df)"]},{"cell_type":"markdown","metadata":{"id":"QzuEqVg3a1ea"},"source":["We will use a `ColumnTransformer`, that allows to process only some particular columns, and leaves the others untouched (pass-through). In this case, we will process only the categorical ones.\n","\n","The output of this transformation **is a Numpy matrix**."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwdnSKoRA9CO"},"outputs":[],"source":["from pprint import pprint\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","categorical_features = ['Sky', 'Windy']\n","\n","preprocessor = ColumnTransformer(\n","    transformers = [\n","                    ('categorical', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n","                   ],\n","                   remainder='passthrough')\n","\n","preprocessor.fit(X_df)\n","X = preprocessor.transform(X_df)\n","\n","# Notice that now we have 7 columnos\n","print(X.shape)\n","print()\n","\n","# Notice that now the type of the data matrix is numpy, which can already be used by sklearn\n","print(type(X))\n","print()\n","\n","# The first three columns are the dummies for Sky, the second two columns are the dummies for Windy\n","# The last two columns are Temperature and Humidity, untouched\n","# Please, notice that the order of columns has changed (not important, in principle)\n","print(X)\n","pprint(list(preprocessor.get_feature_names_out()))"]},{"cell_type":"markdown","metadata":{"id":"bQ7O7ElKoyDp"},"source":["Sometimes it is time consuming to enumerate all categorical columns. We can use `make_column_selector` / `selector` in order to select the types we need."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U4ATnGozohJZ"},"outputs":[],"source":["from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","from sklearn.compose import make_column_selector as selector\n","\n","# numeric_features = ['Temperature', 'Humidity']\n","categorical_features = ['Sky', 'Windy']\n","\n","preprocessor = ColumnTransformer(\n","    transformers = [\n","                    ('categorical', OneHotEncoder(handle_unknown='ignore'), selector(dtype_include=[\"object\",\"bool\"]))\n","                   ],\n","                   remainder='passthrough')\n","\n","preprocessor.fit(X_df)\n","X = preprocessor.transform(X_df)\n","\n","# Notice that now we have 7 columnos\n","print(X.shape)\n","print()\n","\n","# Notice that now the type of the data matrix is numpy, which can already be used by sklearn\n","print(type(X))\n","print()\n","\n","# The first three columns are the dummies for Sky, the second two columns are the dummies for Windy\n","# The last two columns are Temperature and Humidity, untouched\n","# Please, notice that the order of columns has changed (not important, in principle)\n","print(X)"]},{"cell_type":"markdown","metadata":{"id":"wDXkuInIduqR"},"source":["Optionally, we can encode the class into integers. We do that with `LabelEncoder`.\n","\n","**This is not required**, as Scikit-learn can use categorical classes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vfihfozAyP-N"},"outputs":[],"source":["from sklearn import preprocessing\n","le = preprocessing.LabelEncoder()\n","le.fit(y_df)\n","\n","y = le.transform(y_df)\n","print(y)"]},{"cell_type":"markdown","metadata":{"id":"n7w5y9ASdgVF"},"source":["Finally, we just copy the code for evaluating KNN models, and for constructing the final model.\n","We apply that to our (X,y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9sDNvM74QX4"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import neighbors\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Here, we set our model to KNN\n","clf = neighbors.KNeighborsClassifier()\n","\n","# We train it\n","clf.fit(X_train, y_train)\n","\n","# We obtain predictions on the test set\n","y_test_pred = clf.predict(X_test)\n","\n","# We compute accuracy\n","accuracy_knn = metrics.accuracy_score(y_test, y_test_pred)\n","print(f\"Accuracy of the model: {accuracy_knn} \")\n","\n","# We finally compute the final model with all available data\n","final_clf = neighbors.KNeighborsClassifier()\n","final_clf.fit(X, y)"]},{"cell_type":"markdown","metadata":{"id":"Yvrx1JqN4yZm"},"source":["Given that this is a 2-class classification problem, we can construct a confidence interval for the accuracy.\n","\n","We can see it is very inaccurate..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JHaewbCxiBQ6"},"outputs":[],"source":["from statsmodels.stats.proportion import proportion_confint\n","print(f\"Only {len(y_test)} instances on the testing partition\")\n","print(f\"Model accuracy: {accuracy_knn} \")\n","proportion_confint(len(y_test) * accuracy_knn, len(y_test), method=\"wilson\")"]},{"cell_type":"markdown","metadata":{"id":"ADfIN64L4QX4"},"source":["# KNN regression with holdout evaluation"]},{"cell_type":"markdown","metadata":{"id":"L2r3ejoO4QX4"},"source":["Let's load the California housing dataset and check its description. Its data about housing prices depending on the characteristics of the zone."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jRUioZm04QX4"},"outputs":[],"source":["from sklearn.datasets import fetch_california_housing\n","housing = fetch_california_housing()\n","print(housing.DESCR)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4R6sLJoI4QX4"},"outputs":[],"source":["X = housing.data\n","y = housing.target\n","print(X.shape, y.shape)"]},{"cell_type":"markdown","metadata":{"id":"APMXvjj14QX4"},"source":["The main change is that we use a `KNeighborRegressor` and the metric is now RMSE."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZ21Wukc4QX5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import neighbors\n","import numpy as np\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# Here, we set our model to KNN regressor\n","regr = neighbors.KNeighborsRegressor()\n","\n","# We train it\n","regr.fit(X_train, y_train)\n","\n","# We obtain predictions on the test set\n","y_test_pred = regr.predict(X_test)\n","\n","# We compute accuracy\n","rmse_knn = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n","print(f\"RMSE of the model: {rmse_knn}\")\n","\n","# We would have to compute the final model with all available data\n","# Not done here, only interested on test RMSE"]},{"cell_type":"markdown","metadata":{"id":"JZnnKRiQyNOH"},"source":["Is it better than a trivial regressor?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQiGRsbpyN0z"},"outputs":[],"source":["from sklearn.dummy import DummyRegressor\n","regr_mean = DummyRegressor(strategy=\"mean\")\n","regr_mean.fit(X_train, y_train)\n","rmse_mean = np.sqrt(metrics.mean_squared_error(y_test, regr_mean.predict(X_test)))\n","\n","print(f\"RMSE of the KNN model: {rmse_knn}\")\n","print(f\"RMSE of dummy (mean): {rmse_mean}\")\n","print(f\"RMSE ratio KNN/dummy(mean): {rmse_knn/rmse_mean}\")"]},{"cell_type":"markdown","metadata":{"id":"qERBOOPQ0yzI"},"source":["What about MAE?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wJVUu1JO0xiJ"},"outputs":[],"source":["mae_knn = metrics.mean_absolute_error(y_test, y_test_pred)\n","\n","from sklearn.dummy import DummyRegressor\n","regr_median = DummyRegressor(strategy=\"median\")\n","regr_median.fit(X_train, y_train)\n","mae_median = metrics.mean_absolute_error(y_test, regr_median.predict(X_test))\n","\n","print(f\"MAE of the KNN model: {mae_knn}\")\n","print(f\"MAE of dummy (median): {mae_median}\")\n","print(f\"MAE ratio KNN/dummy(median): {mae_knn/mae_median}\")"]},{"cell_type":"markdown","metadata":{"id":"6in65Vuu4QX5"},"source":["The following code contains a complete example on how we can train KNN models for regression:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-rHXrM9B4QX5"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import neighbors\n","\n","# Train/test split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n","\n","# We instantiate the model\n","regr = neighbors.KNeighborsRegressor()\n","\n","# We train it\n","regr.fit(X_train, y_train)\n","\n","# We obtain predictions on the test set\n","y_test_pred = regr.predict(X_test)\n","\n","# We compute accuracy\n","rmse_knn = np.sqrt(metrics.mean_squared_error(y_test, y_test_pred))\n","\n","print(f\"RMSE of the KNN model: {rmse_knn}\")\n","\n","# We would have to compute the final model with all available data\n","# Not done here, only interested on test RMSE"]}],"metadata":{"colab":{"provenance":[]},"interpreter":{"hash":"b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":0}